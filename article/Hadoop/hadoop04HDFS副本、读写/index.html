<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Do Not Stop Your Step">
    <meta name="keyword"  content="BigData  Hadoop Spark Flink">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          HDFS 副本存放策略、文件读写流程 - BigData@Jackson | Blog
        
    </title>

    <link rel="canonical" href="http://www.ruozedata.com/article/Hadoop/hadoop04HDFS副本、读写/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/img/article_header/article_header.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Hadoop" title="Hadoop">Hadoop</a>
                            
                              <a class="tag" href="/tags/#BigData" title="BigData">BigData</a>
                            
                        </div>
                        <h1>HDFS 副本存放策略、文件读写流程</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Jackson on
                            2017-08-20
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/"></a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p><strong>HDFS 副本存放策略、文件读写流程、PID、常规命令、磁盘检查、数据均衡</strong></p>
<hr>
<h3 id="hdfs-副本存放策略">HDFS 副本存放策略</h3>
<p>HDFS默认三副本，在hdfs-site.xml  文件中进行配置，参数 dfs.replication</p>
<p><strong>第一个副本:</strong><br>
假如上传节点为DN节点，优先放置本节点；<br>
否则就随机挑选一台磁盘不太慢、CPU不太繁忙的节点存储副本</p>
<p><strong>第二个副本:</strong><br>
放置在于第一个副本的不同的机架的一个节点上</p>
<p><strong>第三个副本:</strong><br>
放置于第二个副本相同机架的不同节点上</p>
<p>CDH 会有一个虚拟机架，在生产上面我们一般不调整CDH的机架配置</p>
<hr>
<h3 id="hdfs-文件读写流程">HDFS 文件读写流程</h3>
<p>两个核心对象：FSDataOutputStream、FSDataInputStream</p>
<h4 id="文件写流程">文件写流程</h4>
<p>1 Client调用FileSystem.create(filePath)方法，与NN进行【rpc】通信，check是否存在及是否有权限创建；假如不ok，就返回错误信息；假如ok，就创建一个新文件，不关联任何的block块，返回一个FSDataOutputStream对象；</p>
<p>2 Client调用FSDataOutputStream对象的write()方法，先将第一块的第一个副本写到第一个DN，第一个副本写完；就传输给第二个DN，第二个副本写完；就传输给第三个DN，第三个副本写完，就返回一个ack packet确认包给第二个DN，第二个DN接收到第三个的ack  packet确认包加上自身ok，就返回一个ack  packet确认包给第一个DN，第一个DN接收到第二个DN的ack  packet确认包加上自身ok，就返回ack  packet确认包给FSDataOutputStream对象，标志第一个块 3个副本写完。然后余下的块按照上述方式依次写入。</p>
<p>3 当向文件写入数据完成后，<br>
Client调用FSDataOutputStream.close()方法，关闭输出流。</p>
<p>4 再调用FileSystem.complete()方法，告诉NN该文件写入成功。</p>
<hr>
<h4 id="文件读流程">文件读流程</h4>
<p>1 Client调用FileSystem.open(filePath)方法，<br>
与NN进行【rpc】通信，返回该文件的部分或者全部的block列表，<br>
也就是返回FSDataInputStream对象。</p>
<p>2 Client调用FSDataInputStream对象read()方法；</p>
<p>a.与第一个块最近的DN进行read,读取完成后，会check；<br>
假如ok，就关闭与当前DN的通信；假如失败，会记录<br>
失败块+DN信息，下次不会再读取；那么会去该块的<br>
第二个DN地址读取。</p>
<p>b.然后去第二个块的最近的DN上通信读取，check后，关闭通信。</p>
<p>c.假如block列表读取完成后，文件还未结束，<br>
就再次FileSystem会从NN获取该文件的下一批次的block列表。<br>
(感觉就是连续的数据流，对于客户端操作是透明无感知的)</p>
<p>Client调用FSDataInputStream.close()方法，关闭输入流。</p>
<hr>
<h3 id="hadoop-pid">Hadoop PID</h3>
<p>查看pid的配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 hadoop]$ cat hadoop-env.sh |grep -C 10 PID</span><br><span class="line"><span class="comment"># export HADOOP_MOVER_OPTS=""</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment"># Advanced Users Only!</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment"># The directory where pid files are stored. /tmp by default.</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> this should be set to a directory that can only be written to by </span></span><br><span class="line"><span class="comment">#       the user that will run the hadoop daemons.  Otherwise there is the</span></span><br><span class="line"><span class="comment">#       potential for a symlink attack.</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_PID_DIR=<span class="variable">$&#123;HADOOP_PID_DIR&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_SECURE_DN_PID_DIR=<span class="variable">$&#123;HADOOP_PID_DIR&#125;</span></span><br><span class="line"><span class="comment"># A string representing this instance of hadoop. $USER by default.</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_IDENT_STRING=<span class="variable">$USER</span></span><br></pre></td></tr></table></figure>
<p>配置PID的目录，使用下面两个参数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_PID_DIR=/home/hadoop/tmp</span><br><span class="line"><span class="built_in">export</span> HADOOP_SECURE_DN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>
<p><strong><a href="http://yarn-env.sh" target="_blank" rel="noopener">yarn-env.sh</a></strong></p>
<p>export YARN_PID_DIR=/home/hadoop/tmp</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 ~]$ <span class="built_in">cd</span> /home/hadoop/tmp/</span><br><span class="line">[hadoop@bigdata01 tmp]$ ll</span><br><span class="line">drwxrwxr-x 5 hadoop hadoop 51 Dec  1 23:38 dfs</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  6 Dec  5 22:03 hadoop-hadoop-datanode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  6 Dec  5 22:03 hadoop-hadoop-namenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  6 Dec  5 22:03 hadoop-hadoop-secondarynamenode.pid</span><br><span class="line">drwxr-xr-x 5 hadoop hadoop 57 Dec  5 22:06 nm-local-dir</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 42 Dec  2 12:36 test.txt</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  6 Dec  5 22:06 yarn-hadoop-nodemanager.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  6 Dec  5 22:06 yarn-hadoop-resourcemanager.pid</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="hadoop-常规命令">Hadoop 常规命令</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs==&gt; hdfs dfs </span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span><br><span class="line">[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br></pre></td></tr></table></figure>
<p>生产环境需检查是否开启回收站机制，CDH默认开启回收站机制，默认保存七天，七天后自动删除<br>
配置参数如下<br>
<code>fs.trash.interval 10080</code> 分钟  即七天</p>
<p><strong>安全模式：</strong></p>
<p>查看命令帮助</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 hadoop]$ hdfs dfsadmin</span><br><span class="line">Usage: hdfs dfsadmin</span><br><span class="line">Note: Administrative commands can only be run as the HDFS superuser.</span><br><span class="line">	[-report [-live] [-dead] [-decommissioning]]</span><br><span class="line">	[-safemode &lt;enter | leave | get | <span class="built_in">wait</span>&gt;]</span><br></pre></td></tr></table></figure>
<p>使用hdfs dfsadmin -safemode enter 进入安全模式<br>
使用hdfs dfsadmin -safemode leave 离开安全模式</p>
<p>在安全模式下，可以对HDFS进行读操作，但是不能对HDFS进行写操作</p>
<hr>
<h3 id="hdfs-磁盘检查">HDFS 磁盘检查</h3>
<p>hdfs fsck /</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 hadoop]$ hdfs fsck</span><br><span class="line">Usage: DFSck &lt;path&gt; [-list-corruptfileblocks | [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]] [-maintenance]</span><br><span class="line">	&lt;path&gt;	start checking from this path</span><br><span class="line">	-move	move corrupted files to /lost+found</span><br><span class="line">	-delete	delete corrupted files</span><br><span class="line">	-files	<span class="built_in">print</span> out files being checked</span><br><span class="line">	-openforwrite	<span class="built_in">print</span> out files opened <span class="keyword">for</span> write</span><br><span class="line">	-includeSnapshots	include snapshot data <span class="keyword">if</span> the given path indicates a snapshottable directory or there are snapshottable directories under it</span><br><span class="line">	-list-corruptfileblocks	<span class="built_in">print</span> out list of missing blocks and files they belong to</span><br><span class="line">	-blocks	<span class="built_in">print</span> out block report</span><br><span class="line">	-locations	<span class="built_in">print</span> out locations <span class="keyword">for</span> every block</span><br><span class="line">	-racks	<span class="built_in">print</span> out network topology <span class="keyword">for</span> data-node locations</span><br><span class="line">	-maintenance	<span class="built_in">print</span> out maintenance state node details</span><br><span class="line">	-blockId	<span class="built_in">print</span> out <span class="built_in">which</span> file this blockId belongs to, locations (nodes, racks) of this block, and other diagnostics info (under replicated, corrupted or not, etc)</span><br></pre></td></tr></table></figure>
<p><strong>执行命令：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 hadoop]$ hdfs fsck /</span><br><span class="line">19/12/05 22:25:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Connecting to namenode via http://bigdata01:50070/fsck?ugi=hadoop&amp;path=%2F</span><br><span class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.52.50 <span class="keyword">for</span> path / at Thu Dec 05 22:25:45 CST 2019</span><br><span class="line">.........Status: HEALTHY</span><br><span class="line"> Total size:	515494717 B</span><br><span class="line"> Total <span class="built_in">dirs</span>:	13</span><br><span class="line"> Total files:	9</span><br><span class="line"> Total symlinks:		0</span><br><span class="line"> Total blocks (validated):	11 (avg. block size 46863156 B)</span><br><span class="line"> Minimally replicated blocks:	11 (100.0 %)</span><br><span class="line"> Over-replicated blocks:	0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:	0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:		0 (0.0 %)</span><br><span class="line"> Default replication factor:	1</span><br><span class="line"> Average block replication:	1.0</span><br><span class="line"> Corrupt blocks:		0</span><br><span class="line"> Missing replicas:		0 (0.0 %)</span><br><span class="line"> Number of data-nodes:		1</span><br><span class="line"> Number of racks:		1</span><br><span class="line">FSCK ended at Thu Dec 05 22:25:45 CST 2019 <span class="keyword">in</span> 16 milliseconds</span><br><span class="line">The filesystem under path <span class="string">'/'</span> is HEALTHY</span><br></pre></td></tr></table></figure>
<p><strong>以上参数主要看两个部分：</strong></p>
<p>Under-replicated blocks:	0 (0.0 %)<br>
Mis-replicated blocks:		0 (0.0 %)</p>
<hr>
<h3 id="hdfs-数据均衡">HDFS 数据均衡</h3>
<h4 id="各dn节点的数据均衡">各DN节点的数据均衡</h4>
<p><a href="http://xn--start-balancer-4d83a282n.sh" target="_blank" rel="noopener">查看start-balancer.sh</a> 脚本发现使用的是<code>&quot;$bin&quot;/hdfs start balancer</code> 这个命令来执行的</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 sbin]$ <span class="built_in">pwd</span></span><br><span class="line">/home/hadoop/app/hadoop/sbin</span><br><span class="line">[hadoop@bigdata01 sbin]$ cat start-balancer.sh </span><br><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="string">"<span class="variable">$HADOOP_PREFIX</span>"</span>/sbin/hadoop-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script <span class="string">"<span class="variable">$bin</span>"</span>/hdfs start balancer <span class="variable">$@</span></span><br></pre></td></tr></table></figure>
<p>执行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 logs]$ sh /home/hadoop/app/hadoop/sbin/start-balancer.sh </span><br><span class="line">starting balancer, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-bigdata01.out</span><br><span class="line">Time Stamp               Iteration<span class="comment">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></span><br></pre></td></tr></table></figure>
<p>到对应的log文件中查看日志文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata01 logs]$ more hadoop-hadoop-balancer-bigdata01.log</span><br><span class="line">2019-12-05 22:34:50,542 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes  = [hdfs://bigdata01:9000]</span><br><span class="line">2019-12-05 22:34:50,544 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, <span class="comment">#excluded nodes = 0, #included no</span></span><br><span class="line">des = 0, <span class="comment">#source nodes = 0, run during upgrade = false]</span></span><br><span class="line">2019-12-05 22:34:50,544 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: included nodes = []</span><br><span class="line">2019-12-05 22:34:50,544 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: excluded nodes = []</span><br><span class="line">2019-12-05 22:34:50,544 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: <span class="built_in">source</span> nodes = []</span><br><span class="line">2019-12-05 22:34:50,671 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">2019-12-05 22:34:51,708 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)</span><br><span class="line">2019-12-05 22:34:51,709 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)</span><br><span class="line">2019-12-05 22:34:51,709 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)</span><br><span class="line">2019-12-05 22:34:51,709 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)</span><br><span class="line">2019-12-05 22:34:51,714 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)</span><br><span class="line">2019-12-05 22:34:51,726 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.52.50:50010</span><br><span class="line">2019-12-05 22:34:51,727 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 over-utilized: []</span><br><span class="line">2019-12-05 22:34:51,728 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 underutilized: []</span><br></pre></td></tr></table></figure>
<p>可以看到参数：threshold = 10.0<br>
参数的意思是：比如我们有三台的机器，磁盘的使用分别是90 70 60<br>
90+60+80=230/3=76<br>
90-76=14<br>
60-76=16<br>
80-76=4<br>
所有节点的磁盘used与集群的平均used之差要小于这个阈值</p>
<p>dfs.datanode.balance.bandwidthPerSec 30m  参数用于设置带宽</p>
<p>生产上面建议写定时脚本每天执行balance</p>
<h4 id="单个dn节点的多个磁盘的数据均衡">单个DN节点的多个磁盘的数据均衡</h4>
<p>官网参考地址：<code>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</code></p>
<p>df -h<br>
/data01   90%<br>
/data02   60%<br>
/data03   80%<br>
/data04   0%</p>
<p>需要开启这个参数：<code>dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.</code></p>
<p><strong>执行过程：</strong><br>
1.hdfs diskbalance -plan bigdata01	生成bigdata01.plan.json<br>
2.hdfs diskbalance -execute bigdata01.plan.json<br>
3.hdfs diskbalance -query bigdata01</p>
<p><strong>执行场景：</strong><br>
1.新的磁盘加入的时候<br>
2.监控服务器的磁盘剩余空间小于10%的时候，进行执行</p>
<p><strong>DataNode 配置多磁盘目录：</strong><br>
dfs.datanode.data.dir  /data01,/data02/,data03	采用逗号进行分割</p>
<p>为什么DataNode生产上挂多个物理磁盘目录<br>
1.为了高效的写和高效的读<br>
2.可以提前规划好2-3年的存储量，避免后期增加磁盘维护的工作量</p>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/article/Hadoop/Hadoop之MapReduce/" data-toggle="tooltip" data-placement="top" title="MR on Yarn、文件格式、文件压缩">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/article/Error/Link/" data-toggle="tooltip" data-placement="top" title="相关连接地址">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hdfs-副本存放策略"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">HDFS &#x526F;&#x672C;&#x5B58;&#x653E;&#x7B56;&#x7565;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hdfs-文件读写流程"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">HDFS &#x6587;&#x4EF6;&#x8BFB;&#x5199;&#x6D41;&#x7A0B;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#文件写流程"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">&#x6587;&#x4EF6;&#x5199;&#x6D41;&#x7A0B;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#文件读流程"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">&#x6587;&#x4EF6;&#x8BFB;&#x6D41;&#x7A0B;</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hadoop-pid"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Hadoop PID</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hadoop-常规命令"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Hadoop &#x5E38;&#x89C4;&#x547D;&#x4EE4;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hdfs-磁盘检查"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">HDFS &#x78C1;&#x76D8;&#x68C0;&#x67E5;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hdfs-数据均衡"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">HDFS &#x6570;&#x636E;&#x5747;&#x8861;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#各dn节点的数据均衡"><span class="toc-nav-number">6.1.</span> <span class="toc-nav-text">&#x5404;DN&#x8282;&#x70B9;&#x7684;&#x6570;&#x636E;&#x5747;&#x8861;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#单个dn节点的多个磁盘的数据均衡"><span class="toc-nav-number">6.2.</span> <span class="toc-nav-text">&#x5355;&#x4E2A;DN&#x8282;&#x70B9;&#x7684;&#x591A;&#x4E2A;&#x78C1;&#x76D8;&#x7684;&#x6570;&#x636E;&#x5747;&#x8861;</span></a></li></ol></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Hadoop" title="Hadoop">Hadoop</a>
                        
                          <a class="tag" href="/tags/#BigData" title="BigData">BigData</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://nicecloudcode.github.io/" target="_blank">GitHub Blog</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'rz'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<!-- chrome Firefox 中文锚点定位失效-->
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- smooth scroll behavior polyfill  -->
<script type="text/javascript" src="/js/smoothscroll.js"></script>
<script>
        $('#toc').on('click','a',function(a){
            // var isChrome = window.navigator.userAgent.indexOf("Chrome") !== -1;
            // console.log(window.navigator.userAgent,isChrome)
                // if(isChrome) {
                    // console.log(a.currentTarget.outerHTML);
                    // console.log($(a.currentTarget).attr("href"));
                    //跳转到指定锚点
                    // document.getElementById(a.target.innerText.toLowerCase()).scrollIntoView(true);
                    document.getElementById($(a.currentTarget).attr("href").replace("#","")).scrollIntoView({behavior: 'smooth' });
                // }
        })  
</script>


    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/NiceCloudCode">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Jackson 2019 
                    By <a href="https://github.com/NiceCloudCode/NiceCloudCode.github.io">Do Not Stop Your Step!</a> | BigData
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=ruozedata&repo=Bigdata&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://www.ruozedata.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'xxx';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="http://www.ruozedata.com/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
